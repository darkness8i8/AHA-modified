#!/usr/bin/env python3
"""
Main module for the AHA evaluation.
"""
import argparse, logging, os, random, time, json
from pathlib import Path
from typing import List, Dict, Any, Set
import pandas as pd
import threading

import nest_asyncio
nest_asyncio.apply()

from inspect_ai import Task, task, eval
from inspect_ai.dataset import Sample, json_dataset
from inspect_ai.model import Model, get_model, GenerateConfig
from inspect_ai.solver import generate
from scoring import SOLVER_INSTRUCTIONS, final_digit_model_graded_qa
from utils import (
    setup_logging, mkd, read_json, write_json,
    get_latest_file, combine_csv_files
)

# Disable progress display
os.environ["INSPECT_DISABLE_PROGRESS"] = "1"
os.environ["INSPECT_DISABLE_CONSOLE"] = "1"

setup_logging(logging.WARNING)
logging.getLogger("inspect_ai.model").setLevel(logging.WARNING)
config = None
dataset_path = None

# Global variables for timer
timer_running = True
start_time = None
total_batches = None
current_batch = 0

def format_time(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

def timer_thread():
    global timer_running, start_time, total_batches, current_batch
    last_print = 0
    while timer_running:
        current_time = time.time()
        if current_time - last_print >= 60:  # Print every minute
            elapsed = current_time - start_time
            if current_batch > 0 and total_batches > 0:
                avg_time_per_batch = elapsed / current_batch
                remaining_batches = total_batches - current_batch
                estimated_remaining = avg_time_per_batch * remaining_batches
                print(f"\nProgress: {current_batch}/{total_batches} batches")
                print(f"Elapsed: {format_time(elapsed)}")
                print(f"Estimated remaining: {format_time(estimated_remaining)}")
            last_print = current_time
        time.sleep(1)

class Config:
    def __init__(self, args):
        self.model = args.model
        self.judges = args.judges.split(',')
        self.openai_base_url = args.openai_base_url
        self.dataset_path = Path(args.dataset) if args.dataset else Path("data_public.json")
        self.output_dir = Path(args.output_dir) if args.output_dir else Path("results")
        self.batch_size = args.batch_size
        self.seed = args.seed
        self.shuffle = args.shuffle
        self.model_temperature = None if args.model_temperature.lower()=="none" else float(args.model_temperature)
        self.judge_temperature = None if args.judge_temperature.lower()=="none" else float(args.judge_temperature)
        self.run_analysis = args.run_analysis
        self.num_batches = args.num_batches
        self.start_batch = args.start_batch
        self.current_batch = 1
        self.calculate_judge_harshness = args.calculate_judge_harshness
        self.model_kwargs = json.loads(args.model_kwargs) if args.model_kwargs else {}

def setup_environment(conf: Config) -> None:
    mkd(conf.output_dir)
    random.seed(conf.seed)

    os.environ["INSPECT_EVAL_MODEL"] = conf.model
    logging.info(f"INSPECT_EVAL_MODEL set to: '{conf.model}'")
    logging.info(f"AHA Config: Judges='{','.join(conf.judges)}'")
    logging.info(f"AHA Config: Batches={conf.num_batches}x{conf.batch_size}, Seed={conf.seed}")
    logging.info(f"AHA Config: Temp(Model)={conf.model_temperature}, Temp(Judge)={conf.judge_temperature}")

    if conf.model.startswith("vllm/"):
        logging.info("vLLM provider detected. Setting VLLM environment variables for generation params...")

        # Keep temperature and seed env vars for now, though they might be redundant
        # if generate() passes them correctly.
        if conf.model_temperature is not None:
             os.environ["VLLM_TEMPERATURE"] = str(conf.model_temperature)
        if conf.seed is not None:
             os.environ["VLLM_SEED"] = str(conf.seed)

        # Keep TP_SIZE and MAX_TOKENS as they might affect initialization or defaults
        os.environ.setdefault("VLLM_TP_SIZE", "1")
        os.environ.setdefault("VLLM_MAX_TOKENS", "1000")

        vllm_vars = {k: os.environ[k] for k in os.environ if k.startswith("VLLM_")}
        logging.info(f"Remaining VLLM environment variables: {vllm_vars}")

    elif conf.model.startswith("hf/"):
        logging.info("Hugging Face (hf) provider detected. Setting HF environment variables...")
        # os.environ.setdefault("HF_DEVICE", "cuda" if torch.cuda.is_available() else "cpu")
        hf_vars = {k: os.environ[k] for k in os.environ if k.startswith("HF_")}
        logging.info(f"HF environment variables: {hf_vars}")

    if conf.openai_base_url:
         logging.info(f"Setting OPENAI_BASE_URL for potential use by providers (e.g., judges): {conf.openai_base_url}")
         os.environ["OPENAI_BASE_URL"] = conf.openai_base_url
         os.environ.setdefault("OPENAI_API_KEY", "DUMMY_KEY_FOR_ENDPOINT")

    logging.info(f"AHA: {conf.num_batches}x{conf.batch_size}, model={conf.model}")
    logging.info(f"temp={conf.model_temperature}, judge_temp={conf.judge_temperature}, seed={conf.seed}")

def load_and_sample_data(conf: Config, data: List[Dict[str,Any]], used: Set[int]) -> Path:
    if conf.shuffle:
        avail = set(range(len(data))) - used
        if len(avail) < conf.batch_size:
            used.clear(); avail = set(range(len(data)))
        chosen = random.sample(list(avail), conf.batch_size)
    else:
        start = (conf.start_batch + conf.current_batch - 1) * conf.batch_size
        if start >= len(data):
            start = 0; used.clear()
        chosen = list(range(start, min(start + conf.batch_size, len(data))))
    
    used.update(chosen)
    sampled = [data[i] for i in chosen]
    
    tdir = conf.output_dir/"temp"; mkd(tdir)
    sfile = tdir/f"sampled_data_batch{conf.current_batch}.json"
    write_json(sfile, sampled)
    return sfile

def record_to_sample(record: Dict[str,Any]):
    meta = record.get("generation_tags", {}).copy()
    meta["sample_id"] = record["sample_id"]
    meta["tags"] = record.get("tags", [])
    return Sample(
        input=f"{record['question']} Your answer must be EXACTLY 30â€“40 words long.",
        target=SOLVER_INSTRUCTIONS,
        metadata=meta
    )

@task
def aha_evaluation() -> Task:
    try:
        from __main__ import config, dataset_path
    except ImportError:
        global config, dataset_path
    
    ds = json_dataset(str(dataset_path), sample_fields=record_to_sample)
    temp = generate(max_tokens=1000, temperature=config.model_temperature, seed=config.seed, cache=True) if (config.model_temperature and config.seed) else generate(max_tokens=1000, cache=True)
    return Task(
        dataset=ds,
        solver=temp,
        scorer=final_digit_model_graded_qa(config.judges, judge_temperature=config.judge_temperature),
        max_retries=3,
        max_messages=20,
        num_batches=1
    )

def combine_csv_results(conf: Config) -> None:
    cfiles = sorted(conf.output_dir.glob("results_*.csv"))
    if not cfiles:
        logging.error("No CSV files found to combine.")
        return
    df = combine_csv_files(cfiles)
    cols = list(df.columns)
    answer_cols = [c for c in cols if c.endswith("_answer")]
    tag_cols = [c for c in cols if c.startswith("tag")]
    other_cols = [c for c in cols if c not in answer_cols+tag_cols]
    front_cols = []
    for c in ["sample_id","input"]:
        if c in other_cols:
            front_cols.append(c)
            other_cols.remove(c)
    order = front_cols + answer_cols + other_cols + tag_cols
    df = df[order]
    outp = conf.output_dir/"combined_results.csv"
    df.to_csv(outp, index=False)
    logging.info(f"Combined CSV saved to: {outp}")

def main():
    global timer_running, start_time, total_batches, current_batch
    
    p = argparse.ArgumentParser("AHA Benchmark")
    p.add_argument("--model", required=True)
    p.add_argument("--openai-base-url", 
                  help="Base URL for OpenAI-compatible endpoints (e.g., HuggingFace inference endpoints)")
    p.add_argument("--judges", default="anthropic/claude-3-5-sonnet-20241022,google/gemini-1.5-pro-002,openai/gpt-4o-2024-08-06")
    p.add_argument("--dataset")
    p.add_argument("--output_dir")
    p.add_argument("--batch_size", type=int, default=20)
    p.add_argument("--num_batches", type=int, default=1)
    p.add_argument("--seed", type=int, default=42)
    p.add_argument("--shuffle", action="store_true")
    p.add_argument("--model_temperature", default="0.5")
    p.add_argument("--judge_temperature", default="0.5")
    p.add_argument("--run-analysis", action="store_true")
    p.add_argument("--start-batch", type=int, default=0)
    p.add_argument("--calculate_judge_harshness", action="store_true", help="If set, judge harshness is calculated from data.")
    p.add_argument("--model-kwargs", type=str, default=None,
                  help="JSON string of additional kwargs for model initialization")
    args = p.parse_args()

    global config, dataset_path
    config = Config(args)
    os.environ["INSPECT_EVAL_MODEL"] = config.model
    setup_environment(config)

    try:
        full_data = read_json(config.dataset_path)
    except Exception as e:
        logging.error(f"Error loading dataset: {e}")
        return

    used_indices = set()
    sampled_files = []
    start_time = time.time()
    total_batches = config.num_batches
    current_batch = 0

    # Start timer thread
    timer = threading.Thread(target=timer_thread)
    timer.daemon = True
    timer.start()

    try:
        for b in range(config.num_batches):
            current_batch = b + 1
            print(f"Processing batch {current_batch}/{config.num_batches}...")
            
            sfile = load_and_sample_data(config, full_data, used_indices)
            sampled_files.append(sfile)
            dataset_path = sfile
            
            model_args_dict = {}
            if config.model.startswith("vllm/"):
                model_args_dict = {
                    "max_model_len": 8192,
                    "gpu_memory_utilization": 0.95,
                    "trust_remote_code": True
                }

            try:
                eval_result = eval("aha_evaluation", model_args=model_args_dict)
                print(f"âœ“ Batch {current_batch} completed")
            except Exception as e:
                logging.error(f"Error evaluating batch {current_batch}: {e}")
                continue
            
            if config.run_analysis:
                ldir = Path("./logs")
                latest = get_latest_file(ldir, "*.eval")
                if latest:
                    cmd = (
                        f"python analysis.py "
                        f"--log-file {latest} "
                        f"--output-dir {config.output_dir} "
                        f"--solver-name '{config.model}'"
                    )
                    if config.calculate_judge_harshness:
                        cmd += " --calculate_judge_harshness"
                    os.system(cmd)
        
        dt = time.time() - start_time
        timer_running = False  # Stop the timer thread
        print(f"\nTotal time: {format_time(dt)}")
        
        if config.run_analysis:
            print("Combining results...")
            combine_csv_results(config)
            cpath = config.output_dir/"combined_results.csv"
            if cpath.exists():
                print("Running final analysis...")
                cmd = (
                    f"python analysis.py "
                    f"--csv-file {cpath} "
                    f"--output-dir {config.output_dir}"
                )
                os.system(cmd)
    
    finally:
        timer_running = False  # Ensure timer stops even if there's an error
        for f in sampled_files:
            if f.exists():
                f.unlink()

if __name__ == "__main__":
    main()
